{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Input1 | Input2 |   Input3(bias)   | Output        |\n",
    "| :-------------: |:-------------:|\n",
    "| 0 | 0 | 1     | 0 |\n",
    "| 0 | 1 | 1     | 1 |\n",
    "| 1 | 0 | 1     | 1 |\n",
    "| 1 | 1 | 1     | 1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#![title](img/simple_neuralnet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#input data\n",
    "X = np.array([[0,0,1],  \n",
    "            [0,1,1],\n",
    "            [1,0,1]])\n",
    "            #[1,1,1]])\n",
    "# desired output data\n",
    "y = np.array([[0],\n",
    "             [1],\n",
    "             [1]])#,\n",
    "             #[1]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X_test = np.array([[1,1,1]]).T\n",
    "#X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sigmoid\n",
    "def nonlin(x, deriv=False):\n",
    "    if(deriv==True):\n",
    "        return (x*(1-x))\n",
    "    \n",
    "    return 1/(1+np.exp(-x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "#synapses\n",
    "syn0 = 2*np.random.random((3,4)) - 1 \n",
    "# 3x4 matrix of weights ((2 inputs + 1 bias) x 4 nodes in the hidden layer)\n",
    "syn1 = 2*np.random.random((4,1)) - 1  \n",
    "# 4x1 matrix of weights. (4 nodes x 1 output) - no bias term in the hidden layer.\n",
    "\n",
    "\n",
    "syn1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training step\n",
    "for j in range(100000):  \n",
    "    \n",
    "    # Calculate forward through the network.\n",
    "    l0 = X     #layer 0\n",
    "    l1 = nonlin(np.dot(l0, syn0))    #layer 1\n",
    "    l2 = nonlin(np.dot(l1, syn1))    #layer 2\n",
    "\n",
    "    \n",
    "    # Back propagation of errors using the chain rule. \n",
    "    l2_error = y - l2\n",
    "    if( j % 10000 ) == 0:   # Only print the error every 10000 steps, to save time and limit the amount of output. \n",
    "        print (\"Error: \" + str(np.mean(np.abs(l2_error))))\n",
    "        \n",
    "    l2_delta = l2_error*nonlin(l2, deriv=True)\n",
    "    l1_error = l2_delta.dot(syn1.T)\n",
    "    l1_delta = l1_error * nonlin(l1,deriv=True)\n",
    "    \n",
    "    #update weights (no learning rate term)\n",
    "    syn1 += l1.T.dot(l2_delta)\n",
    "    syn0 += l0.T.dot(l1_delta)\n",
    "    \n",
    "print (\"Output after training\")\n",
    "print (l2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l0 = np.array([1,1,1])     #layer 0\n",
    "l1 = nonlin(np.dot(l0, syn0))    #layer 1\n",
    "l2 = nonlin(np.dot(l1, syn1))    #layer 2\n",
    "l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Input1 | Input2 |   Input3 (bias)  | Output        |\n",
    "| :-------------: |:-------------:|\n",
    "| 0 | 0 | 1     | 0 |\n",
    "| 0 | 1 | 1     | 0 |\n",
    "| 1 | 0 | 1     | 1 |\n",
    "| 1 | 1 | 1     | 1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Input1 | Input2 |   Input3   | Output        |\n",
    "| :--: |:--:|:--:|:--:|\n",
    "| 1 | 0 | 0 | ? |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#input data\n",
    "X_train = np.array([[0,0,1],  \n",
    "                    [0,1,1],\n",
    "                    [1,0,1],\n",
    "                    [1,1,1]])\n",
    "\n",
    "y_train = np.array([[0, 1, 1, 0]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "![title](img/singleperceptron.png)\n",
    "\n",
    "### Training process\n",
    "1. Take the inputs from a training set example, adjust them by the weights, and pass them through a special formula to calculate the neuron’s output.\n",
    "2. Calculate the error, which is the difference between the neuron’s output and the desired output in the training set example.\n",
    "3. Depending on the direction of the error, adjust the weights slightly.\n",
    "4. Repeat this process 10, 000 times.\n",
    "\n",
    "### Formula for calculating the neuron’s output\n",
    "```\n",
    "- Sum( weight_i , input_i ) = input1 * weight1 + input2 * weight2 + ... + input_n * weight_n\n",
    "\n",
    "Next we normalise this, so the result is between 0 and 1. Using Sigmoid function: \n",
    "- 1/1+e^x\n",
    "\n",
    "Final Output: \n",
    "-> 1 / 1 + e^(Sum(weights,inputs) \n",
    "```\n",
    "### Adjusting weight\n",
    "\n",
    "1. We used the Sigmoid curve to calculate the output of the neuron.\n",
    "2. If the output is a large positive or negative number, it signifies the neuron was quite confident one way or another.\n",
    "3. From Diagram 4, we can see that at large numbers, the Sigmoid curve has a shallow gradient.\n",
    "4. If the neuron is confident that the existing weight is correct, it doesn’t want to adjust it very much. Multiplying by the Sigmoid curve gradient achieves this.\n",
    "\n",
    "Error Weighted Derivative: \n",
    "``` math\n",
    "Adjust error by = error * inputs * sigmoidCurveGradient(output)\n",
    "sigmoidCurveGradient(output) = output *( 1 - output)\n",
    "Hence => error * inputs * output *( 1 - output)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The Sigmoid function, which describes an S shaped curve.\n",
    "# We pass the weighted sum of the inputs through this function to\n",
    "# normalise them between 0 and 1.\n",
    "def __sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# The derivative of the Sigmoid function.\n",
    "# This is the gradient of the Sigmoid curve.\n",
    "# It indicates how confident we are about the existing weight.\n",
    "def __sigmoid_derivative(x):\n",
    "    return (x*(1-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# We model a single neuron, with 3 input connections and 1 output connection.\n",
    "# We assign random weights to a 3 x 1 matrix, with values in the range -1 to 1\n",
    "# and mean 0.\n",
    "synaptic_weights  = 2 * np.random.random((3, 1))-1 #synaptic_weights \n",
    "# 3x1 matrix of weights ((2 inputs + 1 bias) x 1 nodes in the hidden layer)\n",
    "synaptic_weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# We model a single neuron, with 3 input connections and 1 output connection.\n",
    "# We assign random weights to a 3 x 1 matrix, with values in the range -1 to 1\n",
    "# and mean 0.\n",
    "synaptic_weights  = 2* np.random.random((3, 1))-1 #synaptic_weights \n",
    "# 3x1 matrix of weights ((2 inputs + 1 bias) x 1 nodes in the hidden layer)\n",
    "\n",
    "# training step\n",
    "for j in range(10000):  \n",
    "    \n",
    "    # Calculate forward through the network.\n",
    "    l0 = X_train     #layer 0\n",
    "    output = __sigmoid(np.dot(X_train, synaptic_weights))     #layer 1 <= output layer\n",
    "\n",
    "    # Calculate the error (The difference between the desired output and the predicted output).\n",
    "    # Back propagation of errors using the chain rule. \n",
    "    output_error = y_train - output\n",
    "    if( j % 1000 ) == 0:   # Only print the error every 10000 steps, to save time and limit the amount of output. \n",
    "        print (\"Error: \" + str(np.mean(np.abs(output_error))))\n",
    "\n",
    "            \n",
    "    # Multiply the error by the input and again by the gradient of the Sigmoid curve.\n",
    "    # This means less confident weights are adjusted more.\n",
    "    # This means inputs, which are zero, do not cause changes to the weights.\n",
    "    adjustment = np.dot(X_train.T, output_error * __sigmoid_derivative(output))\n",
    "    \n",
    "    if( j % 1000 ) == 0:   # Only print the error every 10000 steps, to save time and limit the amount of output. \n",
    "        print (\"syn: \", synaptic_weights)\n",
    "            #print(l0.T, l1_error * __sigmoid_derivative(l1))\n",
    "\n",
    "    # Adjust the weights.\n",
    "    synaptic_weights += adjustment\n",
    "    print(synaptic_weights)\n",
    "    \n",
    "    \n",
    "    #l2_delta = l2_error*nonlin(l2, deriv=True)\n",
    "    #l1_error = l2_delta.dot(syn1.T)\n",
    "#     l1_delta = l1_error * nonlin(l1,deriv=True)\n",
    "    \n",
    "#     #update weights (no learning rate term)\n",
    "#     syn1 += l1.T.dot(l2_delta)\n",
    "#     syn0 += l0.T.dot(l1_delta)\n",
    "synaptic_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing as class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import exp, array, random, dot\n",
    "\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self):\n",
    "        # Seed the random number generator, so it generates the same numbers\n",
    "        # every time the program runs.\n",
    "        random.seed(1)\n",
    "\n",
    "        # We model a single neuron, with 3 input connections and 1 output connection.\n",
    "        # We assign random weights to a 3 x 1 matrix, with values in the range -1 to 1\n",
    "        # and mean 0.\n",
    "        self.synaptic_weights = 2 * random.random((3, 1)) - 1\n",
    "\n",
    "    # The Sigmoid function, which describes an S shaped curve.\n",
    "    # We pass the weighted sum of the inputs through this function to\n",
    "    # normalise them between 0 and 1.\n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + exp(-x))\n",
    "\n",
    "    # The derivative of the Sigmoid function.\n",
    "    # This is the gradient of the Sigmoid curve.\n",
    "    # It indicates how confident we are about the existing weight.\n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    # We train the neural network through a process of trial and error.\n",
    "    # Adjusting the synaptic weights each time.\n",
    "    def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations):\n",
    "        for iteration in range(number_of_training_iterations):\n",
    "            # Pass the training set through our neural network (a single neuron).\n",
    "            output = self.think(training_set_inputs)\n",
    "\n",
    "            # Calculate the error (The difference between the desired output\n",
    "            # and the predicted output).\n",
    "            error = training_set_outputs - output\n",
    "            \n",
    "            if( iteration % 1000 ) == 0:   # Only print the error every 10000 steps, to save time and limit the amount of output. \n",
    "                print (\"Error: \" + str(np.mean(np.abs(error))))\n",
    "\n",
    "            # Multiply the error by the input and again by the gradient of the Sigmoid curve.\n",
    "            # This means less confident weights are adjusted more.\n",
    "            # This means inputs, which are zero, do not cause changes to the weights.\n",
    "            adjustment = np.dot(training_set_inputs.T, error * self.__sigmoid_derivative(output))\n",
    "            \n",
    "\n",
    "            # Adjust the weights.\n",
    "            self.synaptic_weights += adjustment\n",
    "            \n",
    "\n",
    "    # The neural network thinks.\n",
    "    def think(self, inputs):\n",
    "        # Pass inputs through our neural network (our single neuron).\n",
    "        return self.__sigmoid(np.dot(inputs, self.synaptic_weights))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #Intialise a single neuron neural network.\n",
    "    neural_network = NeuralNetwork()\n",
    "\n",
    "    print (\"Random starting synaptic weights: \")\n",
    "    print (neural_network.synaptic_weights)\n",
    "\n",
    "    # The training set. We have 4 examples, each consisting of 3 input values\n",
    "    # and 1 output value.\n",
    "    training_set_inputs = array([[0, 0, 1],\n",
    "                                 [1, 1, 1],\n",
    "                                 [1, 0, 1],\n",
    "                                 [0, 1, 1]])\n",
    "    training_set_outputs = array([[0, 1, 1, 0]]).T\n",
    "\n",
    "    # Train the neural network using a training set.\n",
    "    # Do it 10,000 times and make small adjustments each time.\n",
    "    neural_network.train(training_set_inputs, training_set_outputs, 10000)\n",
    "\n",
    "    print (\"New synaptic weights after training: \")\n",
    "    print (neural_network.synaptic_weights)\n",
    "\n",
    "    # Test the neural network with a new situation.\n",
    "    print (\"Considering new situation [1, 0, 0] -> ?: \")\n",
    "    print (neural_network.think(array([1, 0, 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sk learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 5), (5, 2), (2, 1)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "X = [[0., 0.], [1., 1.]]\n",
    "y = [0, 1]\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "clf.fit(X, y)   \n",
    "clf.predict([[1, 0], [0, 1]])\n",
    "[coef.shape for coef in clf.coefs_]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
